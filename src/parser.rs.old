use crate::ast;
use crate::ast::MarkdownNode;
use crate::state;
use crate::tokenizer::{Location, Token, TokenWithLocation, Tokenizer, Whitespace};
use crate::tree::Tree;
use std::fmt::{Debug, Formatter};

pub struct Reader<T>
where
    T: Iterator<Item = TokenWithLocation>,
{
    tokens: T,
    offset: usize,
}
impl<T> Reader<T>
where
    T: Iterator<Item = TokenWithLocation>,
{
    pub fn new(tokens: Vec<TokenWithLocation>) -> Self {
        Self { tokens, offset: 0 }
    }
    pub fn cur(&self) -> usize {
        self.offset
    }
    pub fn len(&self) -> usize {
        self.tokens.len()
    }
    pub fn peek(&self) -> Option<&TokenWithLocation> {
        if self.offset < self.tokens.len() {
            Some(&self.tokens[self.offset])
        } else {
            None
        }
    }
    pub fn get(&self, index: usize) -> Option<&TokenWithLocation> {
        self.tokens.get(index)
    }
    pub fn take(&mut self, len: usize) -> Vec<TokenWithLocation> {
        let start = self.offset;
        let end = self.offset.saturating_add(len);
        if end > self.tokens.len() {
            return Vec::new();
        }
        self.offset = end;
        self.tokens[start..end].to_vec()
    }
    pub fn skip(&mut self, len: usize) {
        self.offset += len
    }
    /// 返回下一个元素
    pub fn next(&mut self) -> Option<&TokenWithLocation> {
        if self.offset < self.tokens.len() {
            let token = &self.tokens[self.offset];
            self.offset += 1;
            Some(token)
        } else {
            None
        }
    }
    /// 查找目标的位置
    pub fn position(&self, mut predicate: impl FnMut(&TokenWithLocation) -> bool) -> Option<usize> {
        self.tokens
            .iter()
            .enumerate()
            .skip(self.offset)
            .find_map(|(index, token)| if predicate(token) { Some(index) } else { None })
    }
    pub fn cur_mut(&mut self) -> &mut usize {
        &mut self.offset
    }
}
impl std::ops::Index<usize> for Reader {
    type Output = TokenWithLocation;
    fn index(&self, index: usize) -> &Self::Output {
        self.tokens.index(index)
    }
}
impl std::ops::Index<std::ops::Range<usize>> for Reader {
    type Output = [TokenWithLocation];
    fn index(&self, range: std::ops::Range<usize>) -> &Self::Output {
        self.tokens.index(range)
    }
}

#[derive(Debug, PartialEq, Eq, Clone)]
pub enum ParseError {
    InvalidImageSyntax { location: Location },
    // 图片语法标题解析失败
    InvalidTitleSyntax { location: Location },
    InvalidInlineCodeSyntax { location: Location },
    UnexpectedHardBreak { location: Location },
    // 意外中断
    UnexpectedInterrupted { location: Location },
    UnexpectedToken { location: Location, token: Token },
    UnexpectedEnded,
    UnexpectedBlock,
    InvalidThematicBreak,
    InvalidATXHeading,
    InvalidIndentedCodeBlock,
    InvalidFencedCodeBlock,
    // 其他错误
    Internal(String),
    UnexpectedEOF,
    UnexpectedIndent,
}

macro_rules! batch_parse_blocks {
    (@apply $self:expr, $($expr:expr),+ $(,)?) => {
        $(
            if $self.is_line_start() && $expr {
                continue;
            }
        )+
        $self.parse_paragraph();
    };
}

impl From<String> for ParseError {
    fn from(value: String) -> Self {
        ParseError::Internal(value)
    }
}

struct Item {
    pub body: MarkdownNode,
    pub start: usize,
    pub end: usize,
}

impl Item {
    fn new(body: MarkdownNode, start: usize, end: usize) -> Self {
        Self { body, start, end }
    }
}

impl Debug for Item {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        self.body.fmt(f)
    }
}

pub struct Parser {
    tree: Tree<Item>,
    last_unclosed_block: Option<usize>,
    inlines: Vec<(usize, std::ops::Range<usize>)>,
}
impl Parser {
    pub fn new(text: &str) -> Self {
        let tokens = Tokenizer::new(text).tokenize();
        let tree = Tree::<Item>::new();
        let reader = Reader::new(tokens);
        Parser {
            tree,
            reader,
            last_unclosed_block: None,
            inlines: Vec::new(),
        }
    }
    fn parse(mut self) -> Tree<Item> {
        self.tree.reset();
        self.tree
            .append(Item::new(MarkdownNode::Document, 0, self.reader.len()));
        self.tree.push();
        self.parse_metadata_block();
        self.parse_blocks();
        self.tree.pop();
        self.tree
    }
    fn parse_blocks(&mut self) {
        while let Some(next) = self.reader.peek() {
            match &next.token {
                Token::Hyphen => {
                    batch_parse_blocks![
                        @apply self,
                        self.recoverable(|parser| {
                            parser.try_parse_setext_heading(ast::HeadingLevel::H2)
                        }),
                        self.recoverable(|parser| {
                            parser.try_parse_thematic_break(Token::Hyphen)
                        })
                    ];
                }
                Token::Eq => {
                    batch_parse_blocks![
                        @apply self,
                        self.recoverable(|parser|parser.try_parse_setext_heading(ast::HeadingLevel::H1))
                    ];
                }
                Token::Asterisk => {
                    batch_parse_blocks![
                        @apply self,
                        self.recoverable(|parser|parser.try_parse_thematic_break(Token::Asterisk))
                    ];
                }
                Token::Crosshatch => {
                    batch_parse_blocks![
                        @apply self,
                        self.recoverable(|parser|parser.try_parse_atx_heading())
                    ];
                }
                Token::Underscore => {
                    batch_parse_blocks![
                        @apply self,
                        self.recoverable(|parser|parser.try_parse_thematic_break(Token::Underscore))
                    ];
                }
                Token::Backtick | Token::Tilde => {
                    let next_token = next.token.clone();
                    batch_parse_blocks![
                        @apply self,
                        self.starts_with(&next_token, 3)
                            && self.recoverable(|parser|parser.try_parse_fenced_code_block(&next_token))
                    ];
                }
                Token::Whitespace(Whitespace::Space) => {
                    batch_parse_blocks![
                        @apply self,
                        self.is_document_level()
                            && self.recoverable(|parser|parser.try_parse_indented_code_block())
                    ];
                }
                Token::Whitespace(Whitespace::NewLine) => {
                    self.reader.next();
                    if self.consume_token(&Token::Whitespace(Whitespace::NewLine)) {
                        self.interrupt_block();
                    } else {
                        self.tree.append(Item::new(
                            MarkdownNode::SoftBreak,
                            self.reader.cur() - 1,
                            self.reader.cur(),
                        ));
                    }
                }
                Token::Whitespace(Whitespace::Comment(_)) => {
                    // ignore comment
                    continue;
                }
                // Token::Escaped('\\') => {
                //     if self.is_line_end()
                //         && recoverable!(self.reader, {
                //             self.reader.next();
                //             if self.consume_token(&Token::Whitespace(Whitespace::NewLine)) {
                //                 self.tree.append(Item::new(
                //                     MarkdownNode::HardBreak,
                //                     self.reader.cur() - 1,
                //                     self.reader.cur(),
                //                 ));
                //                 Some(())
                //             } else {
                //                 None
                //             }
                //         })
                //     {
                //         continue;
                //     }
                //     self.parse_paragraph();
                // }
                _ => {
                    self.parse_paragraph();
                }
            }
        }
    }
    /// 判断当前位置是否处于行开始
    ///
    /// 包含可能存在的3个缩进
    fn is_line_start(&self) -> bool {
        let indent = self.get_current_indent() + 3;
        self.reader[self.reader.cur()].location.column <= indent
    }

    /// 判断当前位置是否处于行结束
    fn is_line_end(&self) -> bool {
        if let Some(next) = self.reader.get(self.reader.cur() + 1) {
            matches!(
                next.token,
                Token::Whitespace(Whitespace::NewLine) | Token::EOF
            )
        } else {
            true
        }
    }

    /// 判断当前是否处于文档级别
    fn is_document_level(&self) -> bool {
        // Document 是第一个插入的，其 IDX 应该等于 Document
        self.tree.peek_up() == Some(0)
    }
    fn starts_with(&self, token: &Token, len: usize) -> bool {
        let start = self.reader.cur();
        if !self.reader[start].is_column_start() {
            return false;
        }
        self.reader[start..start + len]
            .iter()
            .map(|it| &it.token)
            .all(|it| it == token)
    }

    /// 判断未关闭的 Block 的类型是
    fn last_unclosed_block_is(&self, excepted: MarkdownNode) -> bool {
        self.last_unclosed_block
            .and_then(|idx| {
                if self.tree[idx].item.body == excepted {
                    Some(())
                } else {
                    None
                }
            })
            .is_some()
    }
    /// 打断当前块
    fn interrupt_block(&mut self) {
        if self.last_unclosed_block.is_some() {
            self.last_unclosed_block = None;
            while let Some(idx) = self.tree.peek_up() {
                if idx == 0 {
                    break;
                }
                self.tree.pop();
            }
        }
    }

    /// 解析 Markdown FrontMatter Block
    ///
    /// 该方法将在解析开始仅调用一次
    ///
    /// 支持 Yaml Style(`---`) 和 Pluses Style(`+++`) 的标记
    ///
    /// todo: 添加启用 Pluses Style 的Options
    fn parse_metadata_block(&mut self) {
        let marker = match self.reader[0].token {
            Token::Hyphen => Token::Hyphen,
            Token::Plus => Token::Plus,
            _ => return,
        };
        if !self.starts_with(&marker, 3) {
            return;
        }
        self.recoverable(|parser| {
            parser.reader.skip(3);
            let start = parser.reader.cur();
            let mut end = start;
            parser.consume_and_ensure_only_spaces_to_end()?;
            while let Some(next) = parser.reader.peek() {
                if next.is_column_start() && parser.starts_with(&marker, 3) {
                    parser.reader.skip(3);
                    parser.consume_and_ensure_only_spaces_to_end()?;
                    continue;
                }
                end = parser.reader.cur();
                parser.reader.next();
            }
            let tokens = &parser.reader[start..end];
            let text = tokens.iter().map(|it| it.to_string()).collect::<String>();
            parser.tree.create_scope(
                Item::new(MarkdownNode::FrontMatter, start - 3, parser.reader.cur()),
                |tree| {
                    tree.append(Item::new(text.into(), start, end));
                },
            );
            parser.reader.next();
            Ok(())
        });
    }

    /// 解析 Markdown 片段
    fn parse_paragraph(&mut self) {
        let last_unclosed_block = if let Some(idx) = self.last_unclosed_block {
            idx
        } else {
            let idx = self.tree.append(Item::new(
                MarkdownNode::Paragraph,
                self.reader.cur(),
                self.reader.cur() + 1,
            ));
            self.last_unclosed_block = Some(idx);
            self.tree.push();
            idx
        };
        if let Some(r) = self.inlines.last_mut().and_then(|(idx, r)| {
            if idx == &last_unclosed_block {
                Some(r)
            } else {
                None
            }
        }) {
            r.end = self.reader.cur() + 1;
        } else {
            self.inlines.push((
                last_unclosed_block,
                self.reader.cur()..self.reader.cur() + 1,
            ));
        }
        self.reader.next();
    }
    fn try_parse_fenced_code_block(&mut self, marker: &Token) -> Result<(), ParseError> {
        enum State {
            OpenFences,
            LanguageStatement,
            Content,
            CloseFences,
            End,
        }
        let indent = self.ensure_valid_indentation()?;
        let start = self.reader.cur();
        let mut state = State::OpenFences;
        let mut open_fences_num = 0;
        let mut close_fences_num = 0;
        let mut language_start = 0;
        let mut language_end = 0;
        let mut content_start = 0;
        let mut content_end = 0;
        while let Some(next) = self.reader.peek() {
            state = match state {
                State::OpenFences => match &next.token {
                    t @ (Token::Tilde | Token::Backtick) if t == marker => {
                        open_fences_num += 1;
                        State::OpenFences
                    }
                    Token::Whitespace(Whitespace::NewLine) => {
                        language_start = self.reader.cur();
                        language_end = self.reader.cur();
                        content_start = self.reader.cur();
                        State::Content
                    }
                    _ => {
                        language_start = self.reader.cur();
                        State::LanguageStatement
                    }
                },
                State::LanguageStatement => match &next.token {
                    Token::Whitespace(Whitespace::NewLine) => {
                        language_end = self.reader.cur();
                        content_start = self.reader.cur();
                        State::Content
                    }
                    Token::Backtick if marker == &Token::Backtick => {
                        return Err(ParseError::InvalidFencedCodeBlock);
                    }
                    _ => State::LanguageStatement,
                },
                State::Content => {
                    content_end = self.reader.cur();
                    if &next.token == marker && self.is_line_start() {
                        State::CloseFences
                    } else {
                        State::Content
                    }
                }
                State::CloseFences => match &next.token {
                    t @ (Token::Tilde | Token::Backtick) if t == marker => {
                        close_fences_num += 1;
                        State::CloseFences
                    }
                    Token::Whitespace(ws) => {
                        if close_fences_num >= open_fences_num {
                            if ws == &Whitespace::NewLine {
                                State::End
                            } else {
                                State::CloseFences
                            }
                        } else {
                            close_fences_num = 0;
                            State::Content
                        }
                    }
                    _ => {
                        close_fences_num = 0;
                        State::Content
                    }
                },
                State::End => break,
            };
            self.reader.next();
        }
        if !matches!(state, State::End) {
            return Err(ParseError::UnexpectedEOF);
        }
        let language = if language_end > language_start {
            Some(
                self.reader[language_start..language_end]
                    .iter()
                    .map(|it| it.to_string())
                    .collect::<String>(),
            )
        } else {
            None
        };
        let content = self.reader[content_start..content_end]
            .iter()
            .map(|it| it.to_string())
            .collect::<String>();
        self.tree.create_scope(
            Item::new(
                ast::Code {
                    inline: false,
                    language,
                }
                .into(),
                start,
                self.reader.cur(),
            ),
            |tree| {
                tree.append(Item::new(
                    content.trim_end().into(),
                    content_start,
                    content_end,
                ));
            },
        );
        Ok(())
    }

    /// 解析 Markdown 缩进代码块
    fn try_parse_indented_code_block(&mut self) -> Result<(), ParseError> {
        self.reader.next();
        let count = self.consume_until_token(&Token::Whitespace(Whitespace::Space)) + 1;
        if count < 4 {
            return Err(ParseError::InvalidIndentedCodeBlock);
        }
        let start = self.reader.cur() - (count - 4);
        let end = self
            .reader
            .position(|it| it.token == Token::Whitespace(Whitespace::NewLine))
            .unwrap()
            + 1;
        self.reader.skip(end - start);
        let idx = if let Some(idx) = self.last_unclosed_block {
            // update end position
            self.tree[idx].item.end = end;
            idx
        } else {
            let idx = self
                .tree
                .append(Item::new(MarkdownNode::IndentedCodeBlock, start - 4, end));
            self.last_unclosed_block = Some(idx);
            idx
        };
        self.inlines.push((idx, start..end));
        Ok(())
    }

    /// 解析 Markdown ATX 标题
    ///
    /// example: `### heading 3 ### t ####`
    ///
    /// structure: `[hashes counting][content][end hashes?]`
    fn try_parse_atx_heading(&mut self) -> Result<(), ParseError> {
        use state::ATXHeading as State;

        self.ensure_valid_indentation()?;
        let start = self.reader.cur();
        let mut state = State::HashesCounting;
        let mut count = 1;
        let mut content_start = start;
        let mut content_end = start;
        self.reader.next();
        while let Some(next) = self.reader.peek() {
            state = match state {
                State::HashesCounting => match &next.token {
                    Token::Whitespace(Whitespace::Space) => {
                        if count > 6 {
                            return Err(ParseError::InvalidATXHeading);
                        };
                        content_start = self.reader.cur();
                        State::Content
                    }
                    Token::Crosshatch => {
                        count += 1;
                        State::HashesCounting
                    }
                    _ => return Err(ParseError::InvalidATXHeading),
                },
                _ if next.token == Token::Whitespace(Whitespace::NewLine) => State::End,
                State::Content => {
                    content_end = self.reader.cur();
                    match &next.token {
                        Token::Whitespace(Whitespace::Space) => State::EndHashesBefore,
                        _ => State::Content,
                    }
                }
                State::EndHashesBefore => match &next.token {
                    Token::Whitespace(Whitespace::Space) => State::EndHashesBefore,
                    Token::Crosshatch => State::EndHashes,
                    _ => State::Content,
                },
                State::EndHashes => match &next.token {
                    Token::Crosshatch => State::EndHashes,
                    Token::Whitespace(..) => State::EndBefore,
                    _ => State::Content,
                },
                State::EndBefore => match &next.token {
                    Token::Whitespace(..) => State::EndBefore,
                    _ => State::Content,
                },
                State::End => break,
            };
            self.reader.next();
        }
        if !matches!(state, State::End) {
            return Err(ParseError::InvalidATXHeading);
        }
        self.interrupt_block();
        let block = self.tree.append(Item::new(
            ast::HeadingLevel::try_from(count)?.into(),
            start,
            self.reader.cur(),
        ));
        self.inlines.push((block, content_start..content_end));
        Ok(())
    }
    /// 解析 Markdown Setext 标题
    ///
    /// ## 参数
    /// - `level`: `ast::HeadingLevel::H1` 和 `ast::HeadingLevel::H2`
    fn try_parse_setext_heading(&mut self, level: ast::HeadingLevel) -> Result<(), ParseError> {
        if !self.last_unclosed_block_is(MarkdownNode::Paragraph) {
            return Err(ParseError::UnexpectedBlock);
        }
        let idx = self.last_unclosed_block.unwrap();
        self.ensure_valid_indentation()?;
        self.reader.next();
        self.consume_until_token(&if ast::HeadingLevel::H1 == level {
            Token::Eq
        } else {
            Token::Hyphen
        });
        self.consume_and_ensure_only_spaces_to_end()?;
        self.tree.replace(
            idx,
            Item::new(level.into(), self.tree[idx].item.start, self.reader.cur()),
        );
        self.interrupt_block();
        Ok(())
    }
    /// 解析 Markdown 分割线
    ///
    /// ## Params
    /// - `marker`: Token::Asterisk、Token::Hyphen、Token::Underscore
    fn try_parse_thematic_break(&mut self, marker: Token) -> Result<(), ParseError> {
        self.ensure_valid_indentation()?;
        let start = self.reader.cur();
        let mut count = 0u32;
        while let Some(next) = self.reader.peek() {
            match &next.token {
                Token::Asterisk | Token::Hyphen | Token::Underscore => {
                    if marker != next.token {
                        return Err(ParseError::UnexpectedToken {
                            token: next.token.clone(),
                            location: next.location,
                        });
                    }
                    count += 1;
                }
                Token::Whitespace(Whitespace::NewLine) => break,
                // ignore space、tab、comments
                Token::Whitespace(..) => {}
                _ => {
                    return Err(ParseError::UnexpectedToken {
                        token: next.token.clone(),
                        location: next.location,
                    })
                }
            }
            self.reader.next();
        }
        if count < 3 {
            return Err(ParseError::InvalidThematicBreak);
        }
        self.tree.pop();
        self.tree.append(Item::new(
            MarkdownNode::ThematicBreak,
            start,
            self.reader.cur(),
        ));
        self.last_unclosed_block = None;
        Ok(())
    }
    fn parse_inlines(&mut self) {
        todo!()
    }
    fn parse_leaf_block(&mut self) {
        todo!()
    }
    fn parse_container_block(&mut self) {
        todo!()
    }
    // fn parse_node(&mut self) -> Option<()> {
    //     let TokenWithLocation { token, loc } = self.reader.peek()?;
    //     // println!("{:?}@{:?}\n---", token, loc);
    //     match token {
    //         // 可能为标题、标签、标题引用、块引用*、指定属性*、普通文本
    //         Token::Hash => {
    //             // 视作标题
    //             if loc.is_first_col() {
    //                 self.parse_heading();
    //             } else {
    //                 let previous_is_space = self
    //                     .reader
    //                     .get(self.reader.cur() - 1)
    //                     .map(|it| matches!(it.token, Token::Whitespace(Whitespace::Space)))
    //                     .unwrap_or(false);
    //                 let s = token.to_string();
    //                 self.reader.next();
    //                 if previous_is_space {
    //                     self.parse_tag()
    //                 } else {
    //                     self.tree.append(MarkdownNode::Text(s));
    //                 }
    //             }
    //         }
    //         Token::Whitespace(ws) => match ws {
    //             Whitespace::SoftBreak => {
    //                 self.reader.next();
    //                 self.tree.append(MarkdownNode::Text(" ".to_string()));
    //             }
    //             Whitespace::HardBreak => {
    //                 self.reader.next();
    //                 self.tree.pop();
    //                 self.last_unclosed_block = Some(self.tree.append(MarkdownNode::Paragraph));
    //                 self.tree.push();
    //             }
    //             Whitespace::Comment(_) => {
    //                 // 忽略注释
    //                 self.reader.next();
    //             }
    //             Whitespace::Tab => {
    //                 if loc.is_first_col() {
    //                     self.tree
    //                         .replace(self.last_unclosed_block.unwrap(), MarkdownNode::Blockquote);
    //                 } else {
    //                     self.tree
    //                         .append(MarkdownNode::Text(Whitespace::Tab.to_string()));
    //                 }
    //                 self.reader.next();
    //             }
    //             Whitespace::Space => {
    //                 self.reader.next();
    //                 self.tree
    //                     .append(MarkdownNode::Text(Whitespace::Space.to_string()));
    //             }
    //         },
    //         // 可能是 内部链接*
    //         Token::DoubleLBracket => {
    //             self.parse_internal_link();
    //         }
    //         // 可能是 图像、嵌入*
    //         Token::ExclamationMark => {
    //             self.reader.next();
    //             match self.reader.peek().map(|it| &it.token) {
    //                 // 嵌入
    //                 Some(Token::DoubleLBracket) => {
    //                     self.parse_embed();
    //                 }
    //                 // 图像
    //                 Some(Token::LBracket) => {
    //                     self.parse_image();
    //                 }
    //                 // 仅文本
    //                 Some(..) | None => {
    //                     self.tree
    //                         .append(MarkdownNode::Text(Token::ExclamationMark.to_string()));
    //                 }
    //             }
    //         }
    //         Token::Dollar => {
    //             self.parse_math(false);
    //         }
    //         Token::DoubleDollar => {
    //             self.parse_math(true);
    //         }
    //
    //         Token::BackQuote | Token::DoubleBackQuote => {
    //             self.parse_inline_code(token.clone());
    //         }
    //         Token::TripleBackQuote | Token::TripleTilde => {
    //             self.parse_fenced_code(token.clone());
    //         }
    //         Token::EOF => {
    //             // 结束文档
    //         }
    //         Token::Text(s) | Token::Number(s) => {
    //             self.tree.append(s.clone().into());
    //             self.reader.next();
    //         }
    //         _ => {
    //             println!("mismatch token {:?}", token);
    //             self.tree.append(token.to_string().into());
    //             self.reader.next();
    //         }
    //     }
    //     Some(())
    // }
    // fn parse_heading(&mut self) {
    //     let start = self.reader.cur();
    //     let end = self
    //         .reader
    //         .position_until_hardbreak(|it| it.token != Token::Hash)
    //         .unwrap();
    //     let len = end - start;
    //     self.reader.skip(len);
    //     // 超过 6 个，视作普通文本
    //     if len > 6 {
    //         self.tree.append(MarkdownNode::Text("#".repeat(len)));
    //         return;
    //     }
    //     match self.reader.peek().map(|it| it.token.clone()) {
    //         Some(
    //             Token::Whitespace(Whitespace::Space)
    //             | Token::Whitespace(Whitespace::SoftBreak)
    //             | Token::Whitespace(Whitespace::HardBreak)
    //             | Token::EOF,
    //         )
    //         | None => {
    //             self.reader.next();
    //             self.tree.replace(
    //                 self.last_unclosed_block.unwrap(),
    //                 MarkdownNode::Heading(ast::Heading {
    //                     level: ast::HeadingLevel::try_from(len).unwrap(),
    //                 }),
    //             );
    //         }
    //         // 如果紧随的不是空格或换行符以及结尾
    //         _ => {
    //             // 视作标签
    //             if len == 1 {
    //                 self.parse_tag();
    //             } else {
    //                 // 视作文本
    //                 self.tree.append(MarkdownNode::Text("#".repeat(len)));
    //             }
    //         }
    //     }
    // }
    // fn parse_math(&mut self, is_block: bool) {
    //     let start = self.reader.cur();
    //     self.reader.next();
    //     let end = if is_block {
    //         self.reader
    //             .position(|it| matches!(it.token, Token::DoubleDollar))
    //     } else {
    //         self.reader
    //             .position_until_hardbreak(|it| matches!(it.token, Token::Dollar))
    //     };
    //     match end {
    //         Some(end) => {
    //             let len = end - start;
    //             let tokens = self.reader.take(len - 1);
    //             // Block 下不在意前后是否有空白
    //             if is_block {
    //                 self.tree.create_scope(
    //                     MarkdownNode::Math(ast::Math { inline: false }),
    //                     |tree| {
    //                         let text = Parser::tokens_to_string(&tokens[start..end])
    //                             .trim()
    //                             .to_string();
    //                         tree.append(MarkdownNode::Text(text));
    //                     },
    //                 );
    //                 self.reader.next();
    //             }
    //             // Inline 下前后不能是空白，否则视为普通文本
    //             else {
    //                 // 仅仅是 $$，里面没有任何内容
    //                 if tokens.len() == 0 {
    //                     self.tree
    //                         .append(MarkdownNode::Text(Token::DoubleDollar.to_string()));
    //                 }
    //                 // 类似 $ y = x \times 4 $ 这种，前后如果存在空白则视为普通文本
    //                 else if matches!(tokens.first().unwrap().token, Token::Whitespace(..))
    //                     || matches!(tokens.last().unwrap().token, Token::Whitespace(..))
    //                 {
    //                     self.tree.append(MarkdownNode::Text(format!(
    //                         "{}{}{}",
    //                         Token::Dollar.to_string(),
    //                         Parser::tokens_to_string(&tokens[start..end]),
    //                         Token::Dollar.to_string()
    //                     )));
    //                 } else {
    //                     self.tree.create_scope(
    //                         MarkdownNode::Math(ast::Math { inline: true }),
    //                         |tree| {
    //                             let text = Parser::tokens_to_string(&tokens[start..end])
    //                                 .trim()
    //                                 .to_string();
    //                             tree.append(MarkdownNode::Text(text));
    //                         },
    //                     );
    //                 }
    //                 self.reader.next();
    //             }
    //         }
    //         None => {
    //             self.tree.append(MarkdownNode::Text(if is_block {
    //                 Token::DoubleDollar.to_string()
    //             } else {
    //                 Token::Dollar.to_string()
    //             }));
    //         }
    //     }
    // }
    // /// 解析嵌入
    // ///
    // /// examples
    // /// ```markdown
    // /// ![[apple.png]]
    // /// ![[apple.png|300]]
    // /// ![[apple.png|300x300]]
    // /// ![[document.pdf#page=3&height=400]]
    // /// ![[Internal links]]
    // /// ![[Internal links#Heading 1]]
    // /// ![[Internal links#^b15695]]
    // /// ```
    // fn parse_embed(&mut self) {
    //     if let Some(end) = self
    //         .reader
    //         .position_until_hardbreak(|it| matches!(it.token, Token::DoubleRBracket))
    //     {
    //         self.reader.next();
    //         let start = self.reader.cur();
    //         let tokens = self.reader.take(end - start);
    //         if let Some(ref_pos) = tokens.iter().position(|it| {
    //             matches!(it.token, Token::Hash | Token::BlockReference | Token::Pipe)
    //         }) {
    //             let href = Parser::tokens_to_string(&tokens[0..ref_pos]);
    //             let attributes = match tokens.get(ref_pos).unwrap().token {
    //                 Token::Hash => {
    //                     let mut output = Vec::<(String, Option<String>)>::new();
    //                     let mut part = tokens.iter().skip(ref_pos + 1).take(end - ref_pos);
    //                     let mut buffer = String::new();
    //                     while let Some(token) = part.next().map(|it| &it.token) {
    //                         match token {
    //                             Token::Eq => {
    //                                 let key = std::mem::replace(&mut buffer, String::new());
    //                                 while let Some(token) = part.next().map(|it| &it.token) {
    //                                     if matches!(token, Token::Ampersand) {
    //                                         break;
    //                                     }
    //                                     buffer.push_str(&token.to_string());
    //                                 }
    //                                 output.push((key, Some(buffer.to_string())));
    //                                 buffer.clear();
    //                             }
    //                             Token::Ampersand => continue,
    //                             _ => buffer.push_str(&token.to_string()),
    //                         }
    //                     }
    //                     if !buffer.is_empty() {
    //                         output.push((buffer, None));
    //                     }
    //                     output
    //                 }
    //                 Token::BlockReference => {
    //                     vec![(
    //                         String::from("^ref"),
    //                         Some(Parser::tokens_to_string(&tokens[ref_pos + 1..end])),
    //                     )]
    //                 }
    //                 Token::Pipe => {
    //                     match (
    //                         tokens.get(ref_pos + 1).map(|it| &it.token),
    //                         tokens.get(ref_pos + 2).map(|it| &it.token),
    //                         tokens.get(ref_pos + 3).map(|it| &it.token),
    //                     ) {
    //                         (
    //                             Some(Token::Number(width)),
    //                             Some(Token::Text(t)),
    //                             Some(Token::Number(height)),
    //                         ) if t == "x" => {
    //                             vec![
    //                                 (String::from("^width"), Some(width.to_string())),
    //                                 (String::from("^height"), Some(height.to_string())),
    //                             ]
    //                         }
    //                         (Some(Token::Number(width)), None, None) => {
    //                             vec![(String::from("^width"), Some(width.to_string()))]
    //                         }
    //                         _ => vec![],
    //                     }
    //                 }
    //                 _ => {
    //                     vec![]
    //                 }
    //             };
    //             self.tree
    //                 .create_scope(ast::Embed { attributes }.into(), |tree| {
    //                     tree.append(href.into());
    //                 })
    //         } else {
    //             self.tree.create_scope(
    //                 ast::Embed {
    //                     attributes: Vec::new(),
    //                 }
    //                 .into(),
    //                 |tree| {
    //                     tree.append(Parser::tokens_to_string(&tokens).into());
    //                 },
    //             )
    //         }
    //         self.reader.next();
    //     } else {
    //         self.tree
    //             .append(format!("{}{}", Token::ExclamationMark, Token::DoubleLBracket).into());
    //     }
    // }
    // /// 解析图片
    // ///
    // /// examples
    // /// ```markdown
    // /// ![沙漠中的岩石图片](/assets/img/shiprock.jpg)
    // /// ![沙漠中的岩石图片|300](/assets/img/shiprock.jpg)
    // /// ![沙漠中的岩石图片|300x300](/assets/img/shiprock.jpg)
    // /// ![沙漠中的岩石图片|300x300](/assets/img/shiprock.jpg "Shiprock")
    // /// ```
    // fn parse_image(&mut self) {
    //     self.reader.next();
    //     self.reader.create_checkpoint();
    //     let result = self.try_parse_image();
    //     if result.is_err() {
    //         // consumed, append text node
    //         self.tree.append(Token::ExclamationMark.to_string().into());
    //         self.tree.append(Token::LBracket.to_string().into());
    //         self.reader.restore_checkpoint();
    //     }
    // }
    // fn try_parse_image(&mut self) -> Result<(), ParseError> {
    //     let alt_tokens = self.collect_until_token(Token::RBracket)?;
    //     if self.consume_token(Token::RBracket) && self.consume_token(Token::LParen) {
    //         let link_tokens = self.collect_until_token(Token::RParen)?;
    //         let (alt, size) = Parser::extract_label_with_size(&alt_tokens)?;
    //         let (link, title) = Parser::extract_link_with_title(&link_tokens)?;
    //         self.tree
    //             .create_scope(ast::Image { alt, title, size }.into(), |tree| {
    //                 tree.append(link.into());
    //             });
    //         // consume ")"
    //         self.reader.next();
    //         Ok(())
    //     } else {
    //         Err(ParseError::InvalidImageSyntax {
    //             location: self.reader.peek().map(|it| it.location.clone()).unwrap(),
    //         })
    //     }
    // }
    // fn parse_fenced_code(&mut self, mark_token: Token) {
    //     self.reader.next();
    //     if mark_token == Token::TripleBackQuote {
    //         self.reader.create_checkpoint();
    //         let result = self.try_parse_fenced_code(&mark_token);
    //         if result.is_err() {
    //             self.reader.restore_checkpoint();
    //             self.tree.append(mark_token.to_string().into());
    //         }
    //     } else {
    //         self.parse_tilde_fenced_code();
    //     }
    // }
    // fn try_parse_fenced_code(&mut self, mark_token: &Token) -> Result<(), ParseError> {
    //     enum State {
    //         Start,
    //         LanguageStatementStart,
    //         LanguageStatementEnd,
    //         ContentStart,
    //         ContentEnd,
    //         InlineConvert,
    //         End,
    //     }
    //     let mut state = State::Start;
    //     let mut language_tokens = Vec::new();
    //     let mut content_tokens = Vec::new();
    //     let mut fix_fenced_marks = Vec::new();
    //     let mark_token_variants = if mark_token == &Token::TripleBackQuote {
    //         vec![
    //             Token::BackQuote,
    //             Token::DoubleBackQuote,
    //             Token::TripleBackQuote,
    //         ]
    //     } else {
    //         vec![Token::Tilde, Token::DoubleTilde, Token::TripleTilde]
    //     };
    //     while let Some(it) = self.reader.next() {
    //         state = match state {
    //             State::Start => match &it.token {
    //                 Token::Whitespace(Whitespace::SoftBreak) => State::ContentStart,
    //                 Token::Whitespace(Whitespace::HardBreak) => {
    //                     content_tokens.push(Token::Whitespace(Whitespace::SoftBreak));
    //                     State::ContentStart
    //                 }
    //                 Token::Whitespace(..) => State::LanguageStatementEnd,
    //                 // 修正反应号的数量，在结束时将会匹配对应的反应号数量
    //                 t if mark_token_variants.contains(t) => {
    //                     fix_fenced_marks.push(it.token.clone());
    //                     State::Start
    //                 }
    //                 _ => {
    //                     language_tokens.push(it.token.clone());
    //                     State::LanguageStatementStart
    //                 }
    //             },
    //             State::LanguageStatementStart => match it.token {
    //                 Token::Whitespace(Whitespace::SoftBreak) => State::ContentStart,
    //                 Token::Whitespace(Whitespace::HardBreak) => {
    //                     content_tokens.push(Token::Whitespace(Whitespace::SoftBreak));
    //                     State::ContentStart
    //                 }
    //                 Token::Whitespace(..) => State::LanguageStatementEnd,
    //                 Token::BackQuote | Token::DoubleBackQuote => {
    //                     if mark_token == &Token::TripleTilde {
    //                         language_tokens.push(it.token.clone());
    //                         State::LanguageStatementStart
    //                     } else {
    //                         return Err(ParseError::UnexpectedToken {
    //                             location: it.location,
    //                             token: it.token.clone(),
    //                         });
    //                     }
    //                 }
    //                 Token::TripleBackQuote => {
    //                     if mark_token == &Token::TripleTilde {
    //                         language_tokens.push(it.token.clone());
    //                         State::LanguageStatementStart
    //                     } else {
    //                         State::InlineConvert
    //                     }
    //                 }
    //                 _ => {
    //                     language_tokens.push(it.token.clone());
    //                     State::LanguageStatementStart
    //                 }
    //             },
    //             State::LanguageStatementEnd => match it.token {
    //                 Token::Whitespace(Whitespace::SoftBreak) => State::ContentStart,
    //                 Token::Whitespace(Whitespace::HardBreak) => {
    //                     content_tokens.push(Token::Whitespace(Whitespace::SoftBreak));
    //                     State::ContentStart
    //                 }
    //                 _ => State::LanguageStatementEnd,
    //             },
    //             State::ContentStart => {
    //                 // match it.token {
    //                 //     Token::DoubleBackQuote
    //                 // }
    //                 todo!()
    //             }
    //             _ => todo!(),
    //         }
    //     }
    //     todo!()
    // }
    // fn parse_tilde_fenced_code(&mut self) {
    //     enum State {
    //         LanguageStatement,
    //         Content,
    //         MaybeEnd,
    //         End,
    //     }
    //     let mut state = State::LanguageStatement;
    //     let mut language_tokens = Vec::new();
    //     let mut content_tokens = Vec::new();
    //     let mut fix_mark_num = 0;
    //     while let Some(it) = self.reader.next() {
    //         state = match state {
    //             State::LanguageStatement => {
    //                 println!(" -> LanguageStatement {:?}", it.token);
    //                 match &it.token {
    //                     Token::Tilde | Token::DoubleTilde | Token::TripleTilde => {
    //                         fix_mark_num += it.token.quantify();
    //                         State::LanguageStatement
    //                     }
    //                     t @ Token::Whitespace(Whitespace::SoftBreak | Whitespace::HardBreak) => {
    //                         if t == &Token::Whitespace(Whitespace::HardBreak) {
    //                             content_tokens.push(Whitespace::SoftBreak.into());
    //                         }
    //                         State::Content
    //                     }
    //                     _ => {
    //                         language_tokens.push(it.token.clone());
    //                         State::LanguageStatement
    //                     }
    //                 }
    //             }
    //             State::Content => {
    //                 println!(" -> Content {:?}", it.token);
    //                 match it.token {
    //                     Token::TripleTilde => {
    //                         if it.location.is_first_col() {
    //                             self.reader.create_checkpoint();
    //                             State::MaybeEnd
    //                         } else {
    //                             content_tokens.push(it.token.clone());
    //                             State::Content
    //                         }
    //                     }
    //                     Token::EOF => break,
    //                     _ => {
    //                         content_tokens.push(it.token.clone());
    //                         State::Content
    //                     }
    //                 }
    //             }
    //             State::MaybeEnd => {
    //                 println!(" -> MaybeEnd {:?}", it.token);
    //                 if matches!(it.token, Token::EOF) {
    //                     break;
    //                 }
    //                 let next_state = match it.token {
    //                     Token::Tilde | Token::DoubleTilde | Token::TripleTilde => {
    //                         fix_mark_num = fix_mark_num.saturating_sub(it.token.quantify());
    //                         State::MaybeEnd
    //                     }
    //                     Token::Whitespace(Whitespace::SoftBreak | Whitespace::HardBreak) => {
    //                         if fix_mark_num == 0 {
    //                             State::End
    //                         } else {
    //                             State::Content
    //                         }
    //                     }
    //                     Token::Whitespace(..) => {
    //                         if fix_mark_num == 0 {
    //                             State::MaybeEnd
    //                         } else {
    //                             State::Content
    //                         }
    //                     }
    //                     Token::EOF => break,
    //                     _ => State::Content,
    //                 };
    //                 if matches!(next_state, State::Content) {
    //                     println!("-> [Restore]");
    //                     content_tokens.push(Token::TripleTilde);
    //                     self.reader.restore_checkpoint();
    //                 }
    //                 next_state
    //             }
    //             State::End => {
    //                 println!(" -> End {:?}", it.token);
    //                 break;
    //             }
    //         }
    //     }
    //     let language = if language_tokens.is_empty() {
    //         None
    //     } else {
    //         Some(
    //             language_tokens
    //                 .iter()
    //                 .map(|it| it.to_string())
    //                 .collect::<String>(),
    //         )
    //     };
    //     println!("matches = {:?}", matches!(state, State::End));
    //     if matches!(state, State::End) {
    //         content_tokens.pop();
    //     }
    //     println!("content_tokens = {:?}", content_tokens);
    //     let content = content_tokens
    //         .iter()
    //         .map(|it| it.to_string())
    //         .collect::<String>();
    //     self.tree.create_scope(
    //         ast::Code {
    //             inline: false,
    //             language,
    //         }
    //         .into(),
    //         |tree| {
    //             tree.append(content.trim_end().into());
    //         },
    //     );
    // }
    // fn parse_inline_code(&mut self, mark_token: Token) {
    //     self.reader.next();
    //     self.reader.create_checkpoint();
    //     let result = self.try_parse_inline_code(&mark_token);
    //     if result.is_err() {
    //         self.reader.restore_checkpoint();
    //         self.tree.append(mark_token.to_string().into());
    //     }
    // }
    // fn try_parse_inline_code(&mut self, mark_token: &Token) -> Result<(), ParseError> {
    //     let end_position = self.reader.position(|it| &it.token == mark_token);
    //     if let Some(end_position) = end_position {
    //         let tokens = self.reader.take(end_position - self.reader.cur());
    //         let text = Parser::extract_inline_text(&tokens)?;
    //         self.tree.create_scope(
    //             ast::Code {
    //                 inline: true,
    //                 language: None,
    //             }
    //             .into(),
    //             |tree| {
    //                 tree.append(text.into());
    //             },
    //         );
    //         // consume "`" or "``"
    //         self.reader.next();
    //         Ok(())
    //     } else {
    //         Err(ParseError::InvalidInlineCodeSyntax {
    //             location: self.reader.cur_location(),
    //         })
    //     }
    // }
    // fn parse_tag(&mut self) {
    //     match self.reader.next().map(|it| it.token.clone()) {
    //         Some(Token::Hash) => {
    //             let start = self.reader.cur();
    //             let end = self
    //                 .reader
    //                 .position_until_hardbreak(|it| !matches!(it.token, Token::Hash))
    //                 .unwrap_or(start);
    //             let len = end - start;
    //             self.reader.skip(len);
    //             self.tree.append(MarkdownNode::Text("#".repeat(len + 2)));
    //         }
    //         Some(Token::Text(s)) => {
    //             self.tree.create_scope(MarkdownNode::Tag, |tree| {
    //                 tree.append(MarkdownNode::Text(s));
    //             });
    //         }
    //         Some(t) => {
    //             self.tree.append(MarkdownNode::Text("#".to_string()));
    //             self.tree.append(MarkdownNode::Text(t.to_string()));
    //         }
    //         _ => (),
    //     }
    // }
    // fn parse_internal_link(&mut self) {
    //     let start = self.reader.cur();
    //     let end = self
    //         .reader
    //         .position_until_hardbreak(|it| matches!(it.token, Token::DoubleRBracket));
    //     match end {
    //         Some(end) => {
    //             self.reader.next();
    //             let len = end - start;
    //             let tokens = self.reader.take(len - 1);
    //             let label_pos = tokens.iter().position(|it| matches!(it.token, Token::Pipe));
    //             let ref_pos = tokens
    //                 .iter()
    //                 .position(|it| matches!(it.token, Token::Hash | Token::BlockReference));
    //             let (text, reference, label) = match (ref_pos, label_pos) {
    //                 (Some(ref_pos), Some(label_pos)) => {
    //                     // ref 视作无效
    //                     if label_pos < ref_pos {
    //                         (
    //                             Parser::tokens_to_string(&tokens[0..label_pos]),
    //                             None,
    //                             Some(Parser::tokens_to_string(&tokens[label_pos + 1..end])),
    //                         )
    //                     } else {
    //                         let reference = if matches!(
    //                             tokens.get(ref_pos).map(|it| &it.token),
    //                             Some(Token::BlockReference)
    //                         ) {
    //                             ast::Reference::BlockId(Parser::tokens_to_string(
    //                                 &tokens[ref_pos + 1..label_pos],
    //                             ))
    //                         } else {
    //                             ast::Reference::Heading(Parser::tokens_to_string(
    //                                 &tokens[ref_pos + 1..label_pos],
    //                             ))
    //                         };
    //
    //                         let label_str = Parser::tokens_to_string(&tokens[label_pos + 1..end]);
    //                         (
    //                             Parser::tokens_to_string(&tokens[0..ref_pos]),
    //                             Some(reference),
    //                             Some(label_str),
    //                         )
    //                     }
    //                 }
    //                 (Some(ref_pos), None) => {
    //                     let reference = if matches!(
    //                         tokens.get(ref_pos).map(|it| &it.token),
    //                         Some(Token::BlockReference)
    //                     ) {
    //                         ast::Reference::BlockId(Parser::tokens_to_string(
    //                             &tokens[ref_pos + 1..end],
    //                         ))
    //                     } else {
    //                         ast::Reference::Heading(Parser::tokens_to_string(
    //                             &tokens[ref_pos + 1..end],
    //                         ))
    //                     };
    //                     (
    //                         Parser::tokens_to_string(&tokens[0..ref_pos]),
    //                         Some(reference),
    //                         None,
    //                     )
    //                 }
    //                 (None, Some(label_pos)) => (
    //                     Parser::tokens_to_string(&tokens[0..label_pos]),
    //                     None,
    //                     Some(Parser::tokens_to_string(&tokens[label_pos + 1..end])),
    //                 ),
    //                 _ => (Parser::tokens_to_string(&tokens[0..end]), None, None),
    //             };
    //             self.tree.create_scope(
    //                 MarkdownNode::InternalLink(ast::InternalLink { label, reference }),
    //                 move |tree| {
    //                     tree.append(MarkdownNode::Text(text));
    //                 },
    //             );
    //             // skip close
    //             self.reader.next();
    //         }
    //         None => {
    //             self.reader.next();
    //             self.tree
    //                 .append(MarkdownNode::Text(Token::DoubleLBracket.to_string()));
    //         }
    //     }
    // }
    fn consume_token(&mut self, expected: &Token) -> bool {
        if let Some(next) = self.reader.peek() {
            if &next.token == expected {
                self.reader.next();
                true
            } else {
                false
            }
        } else {
            false
        }
    }
    fn consume_until_token(&mut self, expected: &Token) -> usize {
        let mut count = 0;
        while let true = self.consume_token(expected) {
            count += 1;
        }
        count
    }
    fn collect_until_token(&mut self, target: Token) -> Result<Vec<TokenWithLocation>, ParseError> {
        let mut collected = Vec::new();
        while let Some(next) = self.reader.peek() {
            if next.token == target {
                return Ok(collected);
            }
            collected.push(next.clone());
            self.reader.next();
        }
        Err(ParseError::UnexpectedEnded)
    }
    // /// 消费并确认当前位置至行末尾只有空格、制表符
    fn consume_and_ensure_only_spaces_to_end(&mut self) -> Result<(), ParseError> {
        while let Some(next) = self.reader.peek() {
            match next.token {
                Token::Whitespace(Whitespace::NewLine) => {
                    return Ok(());
                }
                Token::Whitespace(..) => {
                    self.reader.next();
                    continue;
                }
                _ => {
                    return Err(ParseError::UnexpectedToken {
                        token: next.token.clone(),
                        location: next.location,
                    })
                }
            }
        }
        Ok(())
    }
    // /// 获取当前缩进
    // ///
    // /// 只有 container block 可以嵌套 block，其他的的 Block 缩进始终为 0
    fn get_current_indent(&self) -> u64 {
        self.last_unclosed_block
            .map(|idx| &self.tree[idx])
            .filter(|node| node.item.body.is_container())
            .map_or(0, |node| self.reader[node.item.end].location.column)
    }
    // /// 确认缩进是否有效
    // ///
    // /// Markdown Block 允许最多三个空格的缩进
    // ///
    // /// ## 返回值
    // ///
    // /// 如果缩进符合规范将返回 `Ok<usize>`,  `usize` 为实际存在的缩进，
    // /// 不符合缩进规范则返回 `Err`
    fn ensure_valid_indentation(&self) -> Result<usize, ParseError> {
        let mut cur = self.reader.cur();
        let mut indent_count = 0;
        let offset_indent = self.get_current_indent();
        for i in 1..=3 {
            if let Some(prev) = self.reader.get(cur.saturating_sub(i)) {
                match &prev.token {
                    Token::Whitespace(Whitespace::Space) => {
                        indent_count += 1;
                        if (prev.location.column.saturating_sub(offset_indent)) <= 1 {
                            return Ok(indent_count);
                        }
                        continue;
                    }
                    _ => {
                        return Err(ParseError::UnexpectedToken {
                            token: prev.token.clone(),
                            location: prev.location,
                        })
                    }
                }
            }
        }
        Err(ParseError::UnexpectedIndent)
    }
    // /// 创建一个从指定索引开始的 token 字符串
    // fn tokens_to_string(tokens: &[TokenWithLocation]) -> String {
    //     tokens
    //         .iter()
    //         .map(|it| match it.token {
    //             Token::Whitespace(Whitespace::NewLine) => "\n".to_string(),
    //             _ => it.token.to_string(),
    //         })
    //         .collect::<String>()
    // }
    // /// 提取标签和 尺寸
    // fn extract_label_with_size(
    //     tokens: &[TokenWithLocation],
    // ) -> Result<(String, Option<(String, String)>), ParseError> {
    //     match tokens.iter().position(|it| it.token == Token::Pipe) {
    //         Some(start) => {
    //             let part = tokens
    //                 .iter()
    //                 .skip(start + 1)
    //                 .filter(|it| !matches!(it.token, Token::Whitespace(Whitespace::Space)))
    //                 .map(|it| &it.token)
    //                 .collect::<Vec<_>>();
    //             match part.len() {
    //                 1 => Ok((
    //                     Parser::extract_inline_text(&tokens[0..start])?,
    //                     Some((part[0].to_string(), part[0].to_string())),
    //                 )),
    //                 3 => match (part[0], part[1], part[2]) {
    //                     (Token::Number(width), Token::Text(ch), Token::Number(height))
    //                         if ch == "x" =>
    //                     {
    //                         Ok((
    //                             Parser::extract_inline_text(&tokens[0..start])?,
    //                             Some((width.clone(), height.clone())),
    //                         ))
    //                     }
    //                     _ => Ok((Parser::extract_inline_text(tokens)?, None)),
    //                 },
    //                 _ => Ok((Parser::tokens_to_string(tokens), None)),
    //             }
    //         }
    //         _ => Ok((Parser::extract_inline_text(tokens)?, None)),
    //     }
    // }
    // /// 提取普通文本，当遇到 Whitespace::HardBreak 会返回错误
    // fn extract_inline_text(tokens: &[TokenWithLocation]) -> Result<String, ParseError> {
    //     if let Some(it) = tokens
    //         .iter()
    //         .find(|it| it.token == Whitespace::HardBreak.into())
    //     {
    //         Err(ParseError::UnexpectedHardBreak {
    //             location: it.location,
    //         })
    //     } else {
    //         Ok(Parser::tokens_to_string(tokens))
    //     }
    // }
    // /// 提取不间断的文本，当遇到 Whitespace 会返回错误
    // fn extract_uninterrupted_text(tokens: &[TokenWithLocation]) -> Result<String, ParseError> {
    //     if let Some(it) = tokens
    //         .iter()
    //         .find(|it| matches!(it.token, Token::Whitespace(..)))
    //     {
    //         Err(ParseError::UnexpectedInterrupted {
    //             location: it.location,
    //         })
    //     } else {
    //         Ok(Parser::tokens_to_string(tokens))
    //     }
    // }
    // /// 提取链接和标题
    // fn extract_link_with_title(
    //     tokens: &[TokenWithLocation],
    // ) -> Result<(String, Option<String>), ParseError> {
    //     let space_position = tokens.iter().position(|it| {
    //         matches!(
    //             it.token,
    //             Token::Whitespace(Whitespace::Space | Whitespace::SoftBreak)
    //         )
    //     });
    //     if let Some(_) = space_position {
    //         enum State {
    //             Start,
    //             LinkStart,
    //             LinkEnd,
    //             TitleStart,
    //             TitleEnd,
    //             Error(ParseError),
    //         }
    //         let mut state = State::Start;
    //         // link range, title range
    //         let mut link_tokens = Vec::new();
    //         let mut title_tokens = Vec::new();
    //         for it in tokens {
    //             state = match state {
    //                 State::Start => match it.token {
    //                     Token::Whitespace(Whitespace::HardBreak) => {
    //                         State::Error(ParseError::UnexpectedHardBreak {
    //                             location: it.location,
    //                         })
    //                     }
    //                     Token::Whitespace(..) => State::Start,
    //                     _ => {
    //                         link_tokens.push(it.clone());
    //                         State::LinkStart
    //                     }
    //                 },
    //                 State::LinkStart => match it.token {
    //                     Token::Whitespace(Whitespace::Space | Whitespace::SoftBreak) => {
    //                         State::LinkEnd
    //                     }
    //                     Token::Whitespace(..) => State::Error(ParseError::UnexpectedInterrupted {
    //                         location: it.location,
    //                     }),
    //                     _ => {
    //                         link_tokens.push(it.clone());
    //                         State::LinkStart
    //                     }
    //                 },
    //                 State::LinkEnd => match it.token {
    //                     Token::Quote => State::TitleStart,
    //                     Token::Whitespace(Whitespace::Space | Whitespace::SoftBreak) => {
    //                         State::LinkEnd
    //                     }
    //                     _ => State::Error(ParseError::InvalidTitleSyntax {
    //                         location: it.location,
    //                     }),
    //                 },
    //                 State::TitleStart => match it.token {
    //                     Token::Quote => State::TitleEnd,
    //                     Token::Whitespace(Whitespace::HardBreak) => {
    //                         State::Error(ParseError::InvalidTitleSyntax {
    //                             location: it.location,
    //                         })
    //                     }
    //                     _ => {
    //                         title_tokens.push(it.clone());
    //                         State::TitleStart
    //                     }
    //                 },
    //                 State::TitleEnd => match it.token {
    //                     Token::Whitespace(Whitespace::Space | Whitespace::SoftBreak) => {
    //                         State::TitleEnd
    //                     }
    //                     _ => State::Error(ParseError::InvalidTitleSyntax {
    //                         location: it.location,
    //                     }),
    //                 },
    //                 State::Error(err) => return Err(err),
    //             }
    //         }
    //         if !matches!(state, State::LinkEnd | State::TitleEnd) {
    //             Err(ParseError::InvalidTitleSyntax {
    //                 location: tokens.last().map(|it| it.location.clone()).unwrap(),
    //             })
    //         } else {
    //             Ok((
    //                 Parser::tokens_to_string(&link_tokens),
    //                 if title_tokens.is_empty() {
    //                     None
    //                 } else {
    //                     Some(Parser::tokens_to_string(&title_tokens))
    //                 },
    //             ))
    //         }
    //     } else {
    //         Ok((Parser::extract_uninterrupted_text(tokens)?, None))
    //     }
    // }
}

trait Recoverable<T> {
    fn recoverable<F>(&mut self, f: F) -> bool
    where
        F: FnOnce(&mut Self) -> T;
}

impl Recoverable<Result<(), ParseError>> for Parser {
    /// 尝试执行给定的函数`f`，如果执行成功（即返回`Ok`），则丢弃记录的还原点；
    /// 如果执行失败（即返回`Err`），则将 TokenReader 指针位置恢复到记录的还原点。
    ///
    /// ## 参数
    ///
    /// - `f`: 一个返回`Result<(), ParseError>`的闭包。这个闭包代表了要尝试执行的解析操作。
    ///
    /// ## 返回值
    ///
    /// - 返回`true`，如果闭包`f`执行成功。
    /// - 返回`false`，如果闭包`f`执行失败。
    fn recoverable<F>(&mut self, f: F) -> bool
    where
        F: FnOnce(&mut Self) -> Result<(), ParseError>,
    {
        let checkpoint = self.reader.cur();
        if f(self).is_ok() {
            true
        } else {
            let _ = std::mem::replace(self.reader.cur_mut(), checkpoint);
            false
        }
    }
}
impl Recoverable<Option<()>> for Parser {
    fn recoverable<F>(&mut self, f: F) -> bool
    where
        F: FnOnce(&mut Self) -> Option<()>,
    {
        let checkpoint = self.reader.cur();
        if f(self).is_some() {
            true
        } else {
            let _ = std::mem::replace(self.reader.cur_mut(), checkpoint);
            false
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    #[test]
    fn test() {
        println!("{:#?}", Parser::new(r#"## Markdown"#).parse())
    }
    // #[test]
    //     fn test_parse_label_with_size() {
    //         let mut t1 = Tokenizer::new("flower.png|30.14150x300").tokenize();
    //         let mut t2 = Tokenizer::new("flower.png|300").tokenize();
    //         let mut t3 = Tokenizer::new("flower.png").tokenize();
    //         // remove EOF token
    //         t1.pop();
    //         t2.pop();
    //         t3.pop();
    //         let result = Parser::extract_label_with_size(&t1);
    //         assert_eq!(
    //             result,
    //             Ok((
    //                 "flower.png".to_string(),
    //                 Some(("30.14150".to_string(), "300".to_string()))
    //             ))
    //         );
    //         let result = Parser::extract_label_with_size(&t2);
    //         assert_eq!(
    //             result,
    //             Ok((
    //                 "flower.png".to_string(),
    //                 Some(("300".to_string(), "300".to_string()))
    //             ))
    //         );
    //         let result = Parser::extract_label_with_size(&t3);
    //         assert_eq!(result, Ok(("flower.png".to_string(), None)));
    //     }
    //     #[test]
    //     fn test_extract_link_with_title() {
    //         assert_eq!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg "Shiprock" "#).tokenize_without_eof(),
    //             ),
    //             Ok((
    //                 "/assets/img/shiprock.jpg".to_string(),
    //                 Some("Shiprock".to_string())
    //             ))
    //         );
    //         assert_eq!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg"#).tokenize_without_eof(),
    //             ),
    //             Ok(("/assets/img/shiprock.jpg".to_string(), None))
    //         );
    //         assert_eq!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg """#).tokenize_without_eof(),
    //             ),
    //             Ok(("/assets/img/shiprock.jpg".to_string(), Some("".to_string())))
    //         );
    //         assert_eq!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"  /assets/img/shiprock.jpg "Shiprock""#).tokenize_without_eof(),
    //             ),
    //             Ok((
    //                 "/assets/img/shiprock.jpg".to_string(),
    //                 Some("Shiprock".to_string())
    //             ))
    //         );
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg ""#).tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new("/assets/img/shiproc k.jpg").tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new("/assets/img/shiproc\nk.jpg").tokenize_without_eof(),
    //             ),
    //             Err(ParseError::UnexpectedInterrupted { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg "Shiprock"#).tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg Shiprock""#).tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg Shiprock"#).tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //         assert!(matches!(
    //             Parser::extract_link_with_title(
    //                 &Tokenizer::new(r#"/assets/img/shiprock.jpg "Shiprock"?"#).tokenize_without_eof(),
    //             ),
    //             Err(ParseError::InvalidTitleSyntax { .. })
    //         ));
    //     }
    //     #[test]
    //     fn test_heading() {
    //         let t1 = Parser::new(r#"# Markdown 标题"#).parse();
    //         let t2 = Parser::new(r#"## Markdown 标题"#).parse();
    //         let t3 = Parser::new(r#"### Markdown 标题"#).parse();
    //         let t4 = Parser::new(r#"#### Markdown 标题"#).parse();
    //         let t5 = Parser::new(r#"##### Markdown 标题"#).parse();
    //         let t6 = Parser::new(r#"###### Markdown 标题"#).parse();
    //         assert_eq!(t1[1], ast::HeadingLevel::H1.into());
    //         assert_eq!(t1[2], "Markdown".into());
    //         assert_eq!(t1[3], " ".into());
    //         assert_eq!(t1[4], "标题".into());
    //         assert_eq!(t2[1], ast::HeadingLevel::H2.into());
    //         assert_eq!(t3[1], ast::HeadingLevel::H3.into());
    //         assert_eq!(t4[1], ast::HeadingLevel::H4.into());
    //         assert_eq!(t5[1], ast::HeadingLevel::H5.into());
    //         assert_eq!(t6[1], ast::HeadingLevel::H6.into());
    //         let t7 = Parser::new(r#"####### Markdown 标题"#).parse();
    //         assert_eq!(t7[1], MarkdownNode::Paragraph);
    //         assert_eq!(t7[2], "#######".into());
    //         let t8 = Parser::new(r#"#Markdown 标题"#).parse();
    //         assert_eq!(t8[1], MarkdownNode::Paragraph);
    //         assert_eq!(t8[2], MarkdownNode::Tag);
    //         assert_eq!(t8[3], "Markdown".into());
    //         let t9 = Parser::new(r#"##Markdown 标题"#).parse();
    //         assert_eq!(t9[1], MarkdownNode::Paragraph);
    //         assert_eq!(t9[2], "##".into());
    //         let t10 = Parser::new(r#" # Markdown 标题"#).parse();
    //         assert_eq!(t10[1], MarkdownNode::Paragraph);
    //         assert_eq!(t10[2], " ".into());
    //         assert_eq!(t10[3], "#".into());
    //         assert_eq!(t10[4], " ".into());
    //         assert_eq!(t10[5], "Markdown".into());
    //         let t11 = Parser::new(r#" ## Markdown 标题"#).parse();
    //         assert_eq!(t11[1], MarkdownNode::Paragraph);
    //         assert_eq!(t11[3], "##".into());
    //         let t12 = Parser::new(r#" #### Markdown 标题"#).parse();
    //         println!("{:#?}", t12);
    //         assert_eq!(t12[1], MarkdownNode::Paragraph);
    //         assert_eq!(t12[3], "####".into());
    //     }
    //     #[test]
    //     fn test_internal_link() {
    //         let t1 = Parser::new(r#"[[hello.txt#Heading 1|喵喵喵]]"#).parse();
    //         assert_eq!(t1[1], MarkdownNode::Paragraph);
    //         assert_eq!(
    //             t1[2],
    //             ast::InternalLink {
    //                 reference: Some(ast::Reference::Heading("Heading 1".to_string())),
    //                 label: Some("喵喵喵".to_string())
    //             }
    //             .into()
    //         );
    //         assert_eq!(t1[3], "hello.txt".into());
    //         let t2 = Parser::new(r#"[[hello.txt"#).parse();
    //         assert_eq!(t2[1], MarkdownNode::Paragraph);
    //         assert_eq!(t2[2], "[[".into());
    //         assert_eq!(t2[3], "hello.txt".into());
    //         let t3 = Parser::new(r#"[[hello.md#^6a247f]]"#).parse();
    //         assert_eq!(
    //             t3[2],
    //             ast::InternalLink {
    //                 reference: Some(ast::Reference::BlockId("6a247f".to_string())),
    //                 label: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t3[3], "hello.md".into())
    //     }
    //     #[test]
    //     fn test_math() {
    //         let t1 = Parser::new(r#"$$ y = x \times 4 $$"#).parse();
    //         assert_eq!(t1[2], ast::Math { inline: false }.into());
    //         assert_eq!(t1[3], r"y = x \times 4".into());
    //         let t2 = Parser::new(r#"$ y = x \times 4 $"#).parse();
    //         let t3 = Parser::new(r#"$y = x \times 4 $"#).parse();
    //         let t4 = Parser::new(r#"$ y = x \times 4$"#).parse();
    //         assert_eq!(t2[2], r"$ y = x \times 4 $".into());
    //         assert_eq!(t3[2], r"$y = x \times 4 $".into());
    //         assert_eq!(t4[2], r"$ y = x \times 4$".into());
    //         let t5 = Parser::new(r#"$y = x \times 4$"#).parse();
    //         assert_eq!(t5[2], ast::Math { inline: true }.into());
    //         assert_eq!(t5[3], r"y = x \times 4".into());
    //         let t6 = Parser::new(
    //             r#"$$
    // y = x \times 4
    // z = x \div 2
    // $$"#,
    //         )
    //         .parse();
    //         let t7 = Parser::new(
    //             r#"$$
    // y = x \times 4
    // z = x \div 2$$"#,
    //         )
    //         .parse();
    //         let t8 = Parser::new(
    //             r#"$$y = x \times 4
    // z = x \div 2$$"#,
    //         )
    //         .parse();
    //         assert_eq!(t6[2], MarkdownNode::Math(ast::Math { inline: false }));
    //         assert_eq!(t6[3], "y = x \\times 4\nz = x \\div 2".into());
    //         assert_eq!(t7[3], "y = x \\times 4\nz = x \\div 2".into());
    //         assert_eq!(t8[3], "y = x \\times 4\nz = x \\div 2".into());
    //     }
    //     #[test]
    //     fn test_code() {
    //         let t1 = Parser::new(r#"`hello`"#).parse();
    //         assert_eq!(
    //             t1[2],
    //             ast::Code {
    //                 inline: true,
    //                 language: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t1[3], "hello".into());
    //         let t2 = Parser::new(r#"`hello"#).parse();
    //         assert_eq!(t2[2], "`".into());
    //         assert_eq!(t2[3], "hello".into());
    //     }
    //     #[test]
    //     fn test_tilde_style_fenced_code() {
    //         let t1 = Parser::new(
    //             r#"~~~~~rust
    //
    // let x = 8u32
    // ~~~~~~~~~~~~~~~~~~~~~~
    // ~~~~"#,
    //         )
    //         .parse();
    //         println!("{:#?}", t1);
    //     }
    //     #[test]
    //     fn test_embed() {
    //         let t = Parser::new("![[Internal links]]").parse();
    //         assert_eq!(t[2], ast::Embed { attributes: vec![] }.into());
    //         assert_eq!(t[3], "Internal links".into());
    //         let t = Parser::new("![[Internal links#Heading 1]]").parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Embed {
    //                 attributes: vec![("Heading 1".to_string(), None)]
    //             }
    //             .into()
    //         );
    //         let t = Parser::new("![[Internal links#^b15695]]").parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Embed {
    //                 attributes: vec![("^ref".to_string(), Some("b15695".to_string()))]
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "Internal links".into());
    //         let t = Parser::new("![[apple.png|300]]").parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Embed {
    //                 attributes: vec![("^width".to_string(), Some("300".to_string()))]
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "apple.png".into());
    //         let t = Parser::new("![[apple.png|300x300]]").parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Embed {
    //                 attributes: vec![
    //                     ("^width".to_string(), Some("300".to_string())),
    //                     ("^height".to_string(), Some("300".to_string()))
    //                 ]
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "apple.png".into());
    //         let t = Parser::new("![[document.pdf#page=3&height=400]]").parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Embed {
    //                 attributes: vec![
    //                     ("page".to_string(), Some("3".to_string())),
    //                     ("height".to_string(), Some("400".to_string()))
    //                 ]
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "document.pdf".into());
    //     }
    //     #[test]
    //     fn test_image() {
    //         // 1. 基本图片语法
    //         let t = Parser::new(r#"![alt-text](/path/to/img.jpg)"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text".to_string(),
    //                 title: None,
    //                 size: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 2. 带有 title 的图片
    //         let t = Parser::new(r#"![alt-text](/path/to/img.jpg "Image Title")"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text".to_string(),
    //                 title: Some("Image Title".to_string()),
    //                 size: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 3. 只带 size，不带 title
    //         let t = Parser::new(r#"![alt-text|100x200](/path/to/img.jpg)"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text".to_string(),
    //                 title: None,
    //                 size: Some(("100".to_string(), "200".to_string()))
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 4. size 和 title 都有
    //         let t = Parser::new(r#"![alt-text|100x200](/path/to/img.jpg "Image Title")"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text".to_string(),
    //                 title: Some("Image Title".to_string()),
    //                 size: Some(("100".to_string(), "200".to_string()))
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 5. 不规范的空格分布
    //         let t = Parser::new(r#"![alt-text|100 x 200 ]( /path/to/img.jpg "Image Title" )"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text".to_string(),
    //                 title: Some("Image Title".to_string()),
    //                 size: Some(("100".to_string(), "200".to_string()))
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 6. 不带 alt 文本
    //         let t = Parser::new(r#"![]( /path/to/img.jpg )"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "".to_string(),
    //                 title: None,
    //                 size: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //
    //         // 7. 不完整的语法
    //         let t = Parser::new(r#"![alt-text|100x](/path/to/img.jpg)"#).parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text|100x".to_string(),
    //                 title: None,
    //                 size: None
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //         // 8. 多个换行和不规范的空格
    //         let t = Parser::new(
    //             r#"![alt-text | 100 x 200 ](
    // /path/to/img.jpg
    // "Image Title"
    // )"#,
    //         )
    //         .parse();
    //         assert_eq!(
    //             t[2],
    //             ast::Image {
    //                 alt: "alt-text ".to_string(),
    //                 title: Some("Image Title".to_string()),
    //                 size: Some(("100".to_string(), "200".to_string()))
    //             }
    //             .into()
    //         );
    //         assert_eq!(t[3], "/path/to/img.jpg".into());
    //     }
}
